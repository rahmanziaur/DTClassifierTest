{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahmanziaur/DTClassifierTest/blob/main/1_CUML_100%20Column%20and%203M.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scfLT2i0MLyD"
      },
      "source": [
        "# Environment Sanity Check #\n",
        "\n",
        "Click the _Runtime_ dropdown at the top of the page, then _Change Runtime Type_ and confirm the instance type is _GPU_.\n",
        "\n",
        "Check the output of `!nvidia-smi` to make sure you've been allocated a Tesla T4, P4, or P100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67T0090Jk2KL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4fbef73-cb88-4ba5-b719-974b2dc8dc05"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Oct 27 12:44:13 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtNdk7PSafKP"
      },
      "source": [
        "#Setup:\n",
        "Set up script installs\n",
        "1. Updates gcc in Colab\n",
        "1. Installs Conda\n",
        "1. Install RAPIDS' current stable version of its libraries, as well as some external libraries including:\n",
        "  1. cuDF\n",
        "  1. cuML\n",
        "  1. cuGraph\n",
        "  1. cuSpatial\n",
        "  1. cuSignal\n",
        "  1. BlazingSQL\n",
        "  1. xgboost\n",
        "1. Copy RAPIDS .so files into current working directory, a neccessary workaround for RAPIDS+Colab integration.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pynvml"
      ],
      "metadata": {
        "id": "N8laUEYt4YwX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aad3edc7-a7a2-4e1c-f4ac-f358db4e88e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pynvml\n",
            "  Downloading pynvml-11.4.1-py3-none-any.whl (46 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 46 kB 3.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: pynvml\n",
            "Successfully installed pynvml-11.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0C8IV5TQnjN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52d02417-ee19-476f-bb5f-7580a5e0fce9"
      },
      "source": [
        "# This get the RAPIDS-Colab install files and test check your GPU.  Run this and the next cell only.\n",
        "# Please read the output of this cell.  If your Colab Instance is not RAPIDS compatible, it will warn you and give you remediation steps.\n",
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/env-check.py"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'rapidsai-csp-utils'...\n",
            "remote: Enumerating objects: 300, done.\u001b[K\n",
            "remote: Counting objects: 100% (129/129), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 300 (delta 74), reused 99 (delta 55), pack-reused 171\u001b[K\n",
            "Receiving objects: 100% (300/300), 87.58 KiB | 7.96 MiB/s, done.\n",
            "Resolving deltas: 100% (136/136), done.\n",
            "***********************************************************************\n",
            "Woo! Your instance has the right kind of GPU, a Tesla T4!\n",
            "***********************************************************************\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sKA_9Dbg4s2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49b83d5f-aa48-40fb-f5f5-786510f4d8d5"
      },
      "source": [
        "# This will update the Colab environment and restart the kernel.  Don't run the next cell until you see the session crash.\n",
        "!bash rapidsai-csp-utils/colab/update_gcc.sh\n",
        "import os\n",
        "os._exit(00)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updating your Colab environment.  This will restart your kernel.  Don't Panic!\n",
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:2 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:5 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Hit:7 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Hit:9 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:10 http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu bionic InRelease [20.8 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1,217 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [3,035 kB]\n",
            "Ign:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:15 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,332 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,467 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,257 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [37.1 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [20.6 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [64.0 kB]\n",
            "Get:22 http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu bionic/main amd64 Packages [50.4 kB]\n",
            "Fetched 11.8 MB in 2s (6,500 kB/s)\n",
            "Reading package lists... Done\n",
            "Added repo\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:6 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:7 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:8 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:9 http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu bionic InRelease\n",
            "Hit:10 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Reading package lists... Done\n",
            "Installing libstdc++\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Selected version '11.1.0-1ubuntu1~18.04.1' (Toolchain test builds:18.04/bionic [amd64]) for 'libstdc++6'\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  gcc-11-base libgcc-s1\n",
            "The following NEW packages will be installed:\n",
            "  gcc-11-base libgcc-s1\n",
            "The following packages will be upgraded:\n",
            "  libstdc++6\n",
            "1 upgraded, 2 newly installed, 0 to remove and 43 not upgraded.\n",
            "Need to get 641 kB of archives.\n",
            "After this operation, 981 kB of additional disk space will be used.\n",
            "Get:1 http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu bionic/main amd64 gcc-11-base amd64 11.1.0-1ubuntu1~18.04.1 [19.0 kB]\n",
            "Get:2 http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu bionic/main amd64 libgcc-s1 amd64 11.1.0-1ubuntu1~18.04.1 [41.8 kB]\n",
            "Get:3 http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu bionic/main amd64 libstdc++6 amd64 11.1.0-1ubuntu1~18.04.1 [580 kB]\n",
            "Fetched 641 kB in 0s (4,517 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package gcc-11-base:amd64.\n",
            "(Reading database ... 123942 files and directories currently installed.)\n",
            "Preparing to unpack .../gcc-11-base_11.1.0-1ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking gcc-11-base:amd64 (11.1.0-1ubuntu1~18.04.1) ...\n",
            "Setting up gcc-11-base:amd64 (11.1.0-1ubuntu1~18.04.1) ...\n",
            "Selecting previously unselected package libgcc-s1:amd64.\n",
            "(Reading database ... 123947 files and directories currently installed.)\n",
            "Preparing to unpack .../libgcc-s1_11.1.0-1ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking libgcc-s1:amd64 (11.1.0-1ubuntu1~18.04.1) ...\n",
            "Replacing files in old package libgcc1:amd64 (1:8.4.0-1ubuntu1~18.04) ...\n",
            "Setting up libgcc-s1:amd64 (11.1.0-1ubuntu1~18.04.1) ...\n",
            "(Reading database ... 123949 files and directories currently installed.)\n",
            "Preparing to unpack .../libstdc++6_11.1.0-1ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking libstdc++6:amd64 (11.1.0-1ubuntu1~18.04.1) over (8.4.0-1ubuntu1~18.04) ...\n",
            "Setting up libstdc++6:amd64 (11.1.0-1ubuntu1~18.04.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1C4pNiBhYHX",
        "outputId": "c6b65389-6fd9-4533-a513-3bb32e8d561c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# This will install CondaColab.  This will restart your kernel one last time.  Run this cell by itself and only run the next cell once you see the session crash.\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è¨ Downloading https://github.com/jaimergp/miniforge/releases/latest/download/Mambaforge-colab-Linux-x86_64.sh...\n",
            "üì¶ Installing...\n",
            "üìå Adjusting configuration...\n",
            "ü©π Patching environment...\n",
            "‚è≤ Done in 0:00:28\n",
            "üîÅ Restarting kernel...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ko_pJZbhZrM",
        "outputId": "5a1a2183-d5cb-4192-df86-38423c1dd472",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# you can now run the rest of the cells as normal\n",
        "import condacolab\n",
        "condacolab.check()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ú®üç∞‚ú® Everything looks OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_2dancFhf70"
      },
      "source": [
        "# Installing RAPIDS is now 'python rapidsai-csp-utils/colab/install_rapids.py <release> <packages>'\n",
        "# The <release> options are 'stable' and 'nightly'.  Leaving it blank or adding any other words will default to stable.\n",
        "!python rapidsai-csp-utils/colab/install_rapids.py stable\n",
        "import os\n",
        "os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n",
        "os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n",
        "os.environ['CONDA_PREFIX'] = '/usr/local'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZJMJ6BulmMn"
      },
      "source": [
        "# RAPIDS is now installed on Colab.  You can copy your code into the cells below.  Enjoy!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "l-Q7e98xCPxY",
        "outputId": "7db751f5-a999-463f-c376-01f43d7a8c48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy dataset 30 times\n",
        "import csv\n",
        "\n",
        "N = 30\n",
        "\n",
        "# /content/drive/MyDrive/0_ColabDatasets/abc_less_feature (2).csv\n",
        "\n",
        "# dataset=pd.read_csv('/content/drive/MyDrive/0_ColabDatasets/abc_less_feature (2).csv')\n",
        "\n",
        "with open('', 'r') as infile,\\\n",
        "         open('/content/drive/MyDrive/Colab Notebooks/Zia Sir/CUML/abc_less_feature30.csv', 'w') as outfile:\n",
        "    reader = csv.reader(infile)\n",
        "    writer = csv.writer(outfile)\n",
        "    writer.writerow(next(reader))    # reads header line and writes it to output file\n",
        "    data = [row for row in reader]    # reads the rest of the input file\n",
        "    for i in range(N):\n",
        "        writer.writerows(data)"
      ],
      "metadata": {
        "id": "Ux5YiLgk8hAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy dataset 30 times\n",
        "import csv\n",
        "\n",
        "N = 30\n",
        "\n",
        "# /content/drive/MyDrive/0_ColabDatasets/abc_less_feature (2).csv\n",
        "\n",
        "# dataset=pd.read_csv('/content/drive/MyDrive/0_ColabDatasets/abc_less_feature (2).csv')\n",
        "\n",
        "with open('/content/drive/MyDrive/0_ColabDatasets/abc_less_feature (2).csv', 'r') as infile,\\\n",
        "         open('/content/drive/MyDrive/0_ColabDatasets/abc_less_feature30.csv', 'w') as outfile:\n",
        "    reader = csv.reader(infile)\n",
        "    writer = csv.writer(outfile)\n",
        "    writer.writerow(next(reader))    # reads header line and writes it to output file\n",
        "    data = [row for row in reader]    # reads the rest of the input file\n",
        "    for i in range(N):\n",
        "        writer.writerows(data)"
      ],
      "metadata": {
        "id": "RXZ8z1AbkZbl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Change the classifier to test the speed of both sklearn and cuml\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "from cuml.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import warnings\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings('ignore')\n",
        "#warnings.filterwarnings('always') \n",
        "\n",
        "# Copy the path of the .csv file produced earlier\n",
        "# dataset=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Zia Sir/20-09-22/abc_less_feature.csv')\n",
        "\n",
        "# 100 k data\n",
        "#dataset=pd.read_csv('/content/drive/MyDrive/0_ColabDatasets/abc_less_feature (2).csv')\n",
        "\n",
        "# 100 k data\n",
        "dataset=pd.read_csv('/content/drive/MyDrive/0_ColabDatasets/abc_less_feature30.csv')\n",
        "\n",
        "print(list(dataset.columns))\n",
        "feature_cols= list(dataset.columns[:-1])\n",
        "\n",
        "target_cols=list(dataset.columns[-1:])\n",
        "#target_cols\n",
        "\n",
        "#split dataset in features and target variable\n",
        "X = dataset.drop('readmitted', axis=1) # Features\n",
        "y = dataset['readmitted'] # Target variable\n",
        "\n",
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "scaler = StandardScaler() \n",
        "X_train = scaler.fit_transform(X_train) \n",
        "X_test = scaler.transform(X_test) \n",
        "# Check the shape of all of these\n",
        "# print(\"X_train shape is : \", X_train.shape)\n",
        "# print(\"X_test shape  is : \", X_test.shape)\n",
        "# print(\"y_train shape is : \", y_train.shape)\n",
        "# print(\"y_test shape is  : \", y_test.shape)\n",
        "\n",
        "#Calculate start time\n",
        "start = timeit.default_timer()\n",
        "\n",
        "#Create a Gaussian Classifier\n",
        "clf=RandomForestClassifier(n_estimators=100)\n",
        "# clf = DecisionTreeClassifier(max_depth = 2, random_state = 0)\n",
        "\n",
        "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "clf.fit(X_train,y_train)\n",
        "\n",
        "#Calculate Stop time\n",
        "stop = timeit.default_timer()\n",
        "train_time= stop - start\n",
        "\n",
        "#Calculate start time\n",
        "start = timeit.default_timer()\n",
        "\n",
        "# Predict the model\n",
        "y_pred=clf.predict(X_test)\n",
        "\n",
        "#Calculate Stop time\n",
        "stop = timeit.default_timer()\n",
        "test_time= stop - start\n",
        "\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "outputId": "75873368-77e1-471e-829c-a2439688e1ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymRr0E8siIm_"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['encounter_id', 'patient_nbr', 'race', 'age', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'time_in_hospital', 'num_lab_procedures', 'num_procedures', 'num_medications', 'number_diagnoses', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed', 'service_utilization', 'numchange', 'readmitted']\n",
            "[[813706      0]\n",
            " [ 75859  26329]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      1.00      0.96    813706\n",
            "           1       1.00      0.26      0.41    102188\n",
            "\n",
            "    accuracy                           0.92    915894\n",
            "   macro avg       0.96      0.63      0.68    915894\n",
            "weighted avg       0.92      0.92      0.89    915894\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Change the classifier to test the speed of both sklearn and cuml\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "from cuml.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import warnings\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings('ignore')\n",
        "#warnings.filterwarnings('always') \n",
        "\n",
        "# Copy the path of the .csv file produced earlier\n",
        "# dataset=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Zia Sir/20-09-22/abc_less_feature.csv')\n",
        "#dataset=pd.read_csv('/content/drive/MyDrive/0_ColabDatasets/abc_less_feature (2).csv')\n",
        "\n",
        "# 100 k data\n",
        "#dataset=pd.read_csv('/content/drive/MyDrive/0_ColabDatasets/abc_less_feature (2).csv')\n",
        "\n",
        "# 100 k data\n",
        "dataset=pd.read_csv('/content/drive/MyDrive/0_ColabDatasets/abc_less_feature30.csv')\n",
        "\n",
        "print(list(dataset.columns))\n",
        "feature_cols= list(dataset.columns[:-1])\n",
        "\n",
        "target_cols=list(dataset.columns[-1:])\n",
        "#target_cols\n",
        "\n",
        "#split dataset in features and target variable\n",
        "X = dataset.drop('readmitted', axis=1) # Features\n",
        "y = dataset['readmitted'] # Target variable\n",
        "\n",
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "scaler = StandardScaler() \n",
        "X_train = scaler.fit_transform(X_train) \n",
        "X_test = scaler.transform(X_test) \n",
        "# Check the shape of all of these\n",
        "# print(\"X_train shape is : \", X_train.shape)\n",
        "# print(\"X_test shape  is : \", X_test.shape)\n",
        "# print(\"y_train shape is : \", y_train.shape)\n",
        "# print(\"y_test shape is  : \", y_test.shape)\n",
        "\n",
        "#Calculate start time\n",
        "start = timeit.default_timer()\n",
        "\n",
        "#Create a Gaussian Classifier\n",
        "clf=RandomForestClassifier(n_estimators=100)\n",
        "# clf = DecisionTreeClassifier(max_depth = 2, random_state = 0)\n",
        "\n",
        "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "clf.fit(X_train,y_train)\n",
        "\n",
        "#Calculate Stop time\n",
        "stop = timeit.default_timer()\n",
        "train_time= stop - start\n",
        "\n",
        "#Calculate start time\n",
        "start = timeit.default_timer()\n",
        "\n",
        "# Predict the model\n",
        "y_pred=clf.predict(X_test)\n",
        "\n",
        "#Calculate Stop time\n",
        "stop = timeit.default_timer()\n",
        "test_time= stop - start\n",
        "\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "outputId": "3570c62e-57aa-4357-e48e-7dc1e5effb8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_ng6vaAiHjE"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['encounter_id', 'patient_nbr', 'race', 'age', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'time_in_hospital', 'num_lab_procedures', 'num_procedures', 'num_medications', 'number_diagnoses', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed', 'service_utilization', 'numchange', 'readmitted']\n",
            "[[813706      0]\n",
            " [ 75859  26329]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      1.00      0.96    813706\n",
            "           1       1.00      0.26      0.41    102188\n",
            "\n",
            "    accuracy                           0.92    915894\n",
            "   macro avg       0.96      0.63      0.68    915894\n",
            "weighted avg       0.92      0.92      0.89    915894\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Change the classifier to test the speed of both sklearn and cuml\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "from cuml.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import warnings\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings('ignore')\n",
        "#warnings.filterwarnings('always') \n",
        "\n",
        "# Copy the path of the .csv file produced earlier\n",
        "dataset=pd.read_csv('/content/drive/MyDrive/0_ColabDatasets/1_HealthDMTool_ 100_column.csv')\n",
        "# dataset=pd.read_csv('https://github.com/mustain-billah/healthdmtool/blob/6d63e39aa759b63befd3f3889a25f10a96a4e00f/abc_less_feature%20(2).csv')\n",
        "# dataset=pd.read_csv('https://github.com/mustain-billah/healthdmtool/blob/0131e11571328f61566c3637f5839e06a6d38f08/HealthDMTool_%20100_column.csv')\n",
        "\n",
        "\n",
        "print(list(dataset.columns))\n",
        "feature_cols= list(dataset.columns[:-1])\n",
        "\n",
        "target_cols=list(dataset.columns[-1:])\n",
        "#target_cols\n",
        "\n",
        "#split dataset in features and target variable\n",
        "X = dataset.drop('readmitted', axis=1) # Features\n",
        "y = dataset['readmitted'] # Target variable\n",
        "\n",
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "scaler = StandardScaler() \n",
        "X_train = scaler.fit_transform(X_train) \n",
        "X_test = scaler.transform(X_test) \n",
        "# Check the shape of all of these\n",
        "# print(\"X_train shape is : \", X_train.shape)\n",
        "# print(\"X_test shape  is : \", X_test.shape)\n",
        "# print(\"y_train shape is : \", y_train.shape)\n",
        "# print(\"y_test shape is  : \", y_test.shape)\n",
        "\n",
        "#Calculate start time\n",
        "start = timeit.default_timer()\n",
        "\n",
        "#Create a Gaussian Classifier\n",
        "clf=RandomForestClassifier(n_estimators=100)\n",
        "# clf = DecisionTreeClassifier(max_depth = 2, random_state = 0)\n",
        "\n",
        "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "clf.fit(X_train,y_train)\n",
        "\n",
        "#Calculate Stop time\n",
        "stop = timeit.default_timer()\n",
        "train_time= stop - start\n",
        "\n",
        "#Calculate start time\n",
        "start = timeit.default_timer()\n",
        "\n",
        "# Predict the model\n",
        "y_pred=clf.predict(X_test)\n",
        "\n",
        "#Calculate Stop time\n",
        "stop = timeit.default_timer()\n",
        "test_time= stop - start\n",
        "\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))"
      ],
      "metadata": {
        "id": "VMcgynNICNj1",
        "outputId": "2fef46af-375a-4bd2-d1dc-8cb04027231b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['encounter_id', 'patient_nbr', 'race', 'age', 'admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'time_in_hospital', 'num_lab_procedures', 'num_procedures', 'num_medications', 'number_diagnoses', 'max_glu_serum', 'A1Cresult', 'change', 'diabetesMed', 'service_utilization', 'numchange', 'encounter_id.1', 'patient_nbr.1', 'race.1', 'age.1', 'admission_type_id.1', 'discharge_disposition_id.1', 'admission_source_id.1', 'time_in_hospital.1', 'num_lab_procedures.1', 'num_procedures.1', 'num_medications.1', 'number_diagnoses.1', 'max_glu_serum.1', 'A1Cresult.1', 'change.1', 'diabetesMed.1', 'service_utilization.1', 'numchange.1', 'encounter_id.2', 'patient_nbr.2', 'race.2', 'age.2', 'admission_type_id.2', 'discharge_disposition_id.2', 'admission_source_id.2', 'time_in_hospital.2', 'num_lab_procedures.2', 'num_procedures.2', 'num_medications.2', 'number_diagnoses.2', 'max_glu_serum.2', 'A1Cresult.2', 'change.2', 'diabetesMed.2', 'service_utilization.2', 'numchange.2', 'encounter_id.3', 'patient_nbr.3', 'race.3', 'age.3', 'admission_type_id.3', 'discharge_disposition_id.3', 'admission_source_id.3', 'time_in_hospital.3', 'num_lab_procedures.3', 'num_procedures.3', 'num_medications.3', 'number_diagnoses.3', 'max_glu_serum.3', 'A1Cresult.3', 'change.3', 'diabetesMed.3', 'service_utilization.3', 'numchange.3', 'encounter_id.4', 'patient_nbr.4', 'race.4', 'age.4', 'admission_type_id.4', 'discharge_disposition_id.4', 'admission_source_id.4', 'time_in_hospital.4', 'num_lab_procedures.4', 'num_procedures.4', 'num_medications.4', 'number_diagnoses.4', 'max_glu_serum.4', 'A1Cresult.4', 'change.4', 'diabetesMed.4', 'service_utilization.4', 'numchange.4', 'num_procedures.5', 'num_medications.5', 'number_diagnoses.5', 'max_glu_serum.5', 'A1Cresult.5', 'change.5', 'diabetesMed.5', 'service_utilization.5', 'numchange.5', 'readmitted']\n",
            "[[27133    17]\n",
            " [ 3354    26]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      1.00      0.94     27150\n",
            "           1       0.60      0.01      0.02      3380\n",
            "\n",
            "    accuracy                           0.89     30530\n",
            "   macro avg       0.75      0.50      0.48     30530\n",
            "weighted avg       0.86      0.89      0.84     30530\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_time"
      ],
      "metadata": {
        "id": "BzVwy-5MESTR",
        "outputId": "6c1f2270-e0ae-4c4c-a094-8e30354c90a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0290237640001578"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_time"
      ],
      "metadata": {
        "id": "sks8sNZ2IYNK",
        "outputId": "71a90a40-92b3-40cb-b59f-91d5df88d785",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.9814098400001967"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dlsyk9m9NN2K"
      },
      "source": [
        "# Next Steps #\n",
        "\n",
        "For an overview of how you can access and work with your own datasets in Colab, check out [this guide](https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92).\n",
        "\n",
        "For more RAPIDS examples, check out our RAPIDS notebooks repos:\n",
        "1. https://github.com/rapidsai/notebooks\n",
        "2. https://github.com/rapidsai/notebooks-contrib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nLrk46BllED"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code is for comparing the speed of sklearn and cuml\n",
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "X, y  = datasets.make_classification(n_samples=40000)\n",
        "X = X.astype(np.float32)\n",
        "y = y.astype(np.float32)\n",
        "def train_data(model, X=X, y=y):\n",
        "    clf = model\n",
        "    clf.fit(X, y)\n",
        "from sklearn.svm import SVC \n",
        "from cuml.svm import SVC as SVC_gpu\n",
        "\n",
        "clf_svc = SVC(kernel='poly', degree=2, gamma='auto', C=1)\n",
        "sklearn_time_svc = %timeit -o train_data(clf_svc)\n",
        "\n",
        "clf_svc = SVC_gpu(kernel='poly', degree=2, gamma='auto', C=1)\n",
        "cuml_time_svc = %timeit -o train_data(clf_svc)\n",
        "\n",
        "print(f\"\"\"Average time of sklearn's {clf_svc.__class__.__name__}\"\"\", sklearn_time_svc.average, 's')\n",
        "print(f\"\"\"Average time of cuml's {clf_svc.__class__.__name__}\"\"\", cuml_time_svc.average, 's')\n",
        "\n",
        "print('Ratio between sklearn and cuml is', sklearn_time_svc.average/cuml_time_svc.average)"
      ],
      "metadata": {
        "id": "bGN7AjLs_Aph",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83fa166d-a639-4d4a-b77c-f2f0e3b88125"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1min 12s ¬± 458 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n",
            "2.91 s ¬± 16 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n",
            "Average time of sklearn's SVC 72.62632194728569 s\n",
            "Average time of cuml's SVC 2.9128898112857575 s\n",
            "Ratio between sklearn and cuml is 24.9327391876963\n"
          ]
        }
      ]
    }
  ]
}