{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahmanziaur/DTClassifierTest/blob/main/1_Summary_Sci_Cuml_RF_43_100_3M.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktV6rfKD-tlD",
        "outputId": "9b9e8351-b36f-4561-932a-85e60cf39b26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# for mounting drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **sklearn Random forest for 1 Million dataset with 43 columns**"
      ],
      "metadata": {
        "id": "f43u45wHMilO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sklearn Random forest for 1 Million dataset with 43 columns\n",
        "\n",
        "import timeit\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Change the classifier to test the speed of both sklearn and cuml\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import warnings\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# sklearn_1M_43_columns_\n",
        "# sklearn_3M_100_columns_\n",
        "# CUML_1M_43_columns_\n",
        "# CUML_3M_100_columns_\n",
        "\n",
        "# /content/drive/MyDrive/0_ColabDatasets/1M_dataset_100_columns.csv\n",
        "dataset=pd.read_csv('/content/drive/MyDrive/0_ColabDatasets/1M_dataset_43_columns.csv')\n",
        "\n",
        "feature_cols= list(dataset.columns[:-1])\n",
        "\n",
        "target_cols=list(dataset.columns[-1:])\n",
        "#target_cols\n",
        "\n",
        "#split dataset in features and target variable\n",
        "X = dataset.drop('readmitted', axis=1) # Features\n",
        "y = dataset['readmitted'] # Target variable\n",
        "\n",
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "scaler = StandardScaler() \n",
        "X_train = scaler.fit_transform(X_train) \n",
        "X_test = scaler.transform(X_test) \n",
        "\n",
        "#Calculate start time\n",
        "start = timeit.default_timer()\n",
        "\n",
        "#Create a Gaussian Classifier\n",
        "clf=RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "clf.fit(X_train,y_train)\n",
        "\n",
        "#Calculate Stop time\n",
        "stop = timeit.default_timer()\n",
        "train_time= stop - start\n",
        "\n",
        "#Calculate start time\n",
        "start = timeit.default_timer()\n",
        "\n",
        "# Predict the model\n",
        "y_pred=clf.predict(X_test)\n",
        "\n",
        "#Calculate Stop time\n",
        "stop = timeit.default_timer()\n",
        "test_time= stop - start\n",
        "\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "\n",
        "sklearn_1M_43_columns_train_time = train_time\n",
        "sklearn_1M_43_columns_shape = dataset.shape"
      ],
      "metadata": {
        "id": "ymRr0E8siIm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nF40sGpmRQdb",
        "outputId": "85d01f0b-9ac3-4272-be9b-d1ef4708039b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **sklearn Random forest for 3 Million dataset with 100 columns**"
      ],
      "metadata": {
        "id": "L0hhyzhTMdOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sklearn Random forest for 3 Million dataset with 100 columns\n",
        "\n",
        "import timeit\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Change the classifier to test the speed of both sklearn and cuml\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import warnings\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "dataset=pd.read_csv('/content/drive/MyDrive/0_ColabDatasets/3M_dataset_100_columns.csv')\n",
        "\n",
        "print(list(dataset.columns))\n",
        "feature_cols= list(dataset.columns[:-1])\n",
        "\n",
        "target_cols=list(dataset.columns[-1:])\n",
        "#target_cols\n",
        "\n",
        "#split dataset in features and target variable\n",
        "X = dataset.drop('readmitted', axis=1) # Features\n",
        "y = dataset['readmitted'] # Target variable\n",
        "\n",
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "scaler = StandardScaler() \n",
        "X_train = scaler.fit_transform(X_train) \n",
        "X_test = scaler.transform(X_test) \n",
        "\n",
        "#Calculate start time\n",
        "start = timeit.default_timer()\n",
        "\n",
        "#Create a Gaussian Classifier\n",
        "clf=RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "clf.fit(X_train,y_train)\n",
        "\n",
        "#Calculate Stop time\n",
        "stop = timeit.default_timer()\n",
        "train_time= stop - start\n",
        "\n",
        "#Calculate start time\n",
        "start = timeit.default_timer()\n",
        "\n",
        "# Predict the model\n",
        "y_pred=clf.predict(X_test)\n",
        "\n",
        "#Calculate Stop time\n",
        "stop = timeit.default_timer()\n",
        "test_time= stop - start\n",
        "\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "\n",
        "sklearn_3M_100_columns_train_time = train_time\n",
        "sklearn_3M_100_columns_shape = dataset.shape"
      ],
      "metadata": {
        "id": "FeWyRWPnF4Ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Necessary packages and instalation for running CUML in colab**"
      ],
      "metadata": {
        "id": "FFTrY8arMTPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pynvml"
      ],
      "metadata": {
        "id": "rH9AvO_JLxQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This get the RAPIDS-Colab install files and test check your GPU.  Run this and the next cell only.\n",
        "# Please read the output of this cell.  If your Colab Instance is not RAPIDS compatible, it will warn you and give you remediation steps.\n",
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/env-check.py"
      ],
      "metadata": {
        "id": "qGxRwjoTL0pC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will update the Colab environment and restart the kernel.  Don't run the next cell until you see the session crash.\n",
        "!bash rapidsai-csp-utils/colab/update_gcc.sh\n",
        "import os\n",
        "os._exit(00)"
      ],
      "metadata": {
        "id": "zn_Ad_LzL4Y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This will install CondaColab.  This will restart your kernel one last time.  Run this cell by itself and only run the next cell once you see the session crash.\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "id": "-OJA3flEL7Dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you can now run the rest of the cells as normal\n",
        "import condacolab\n",
        "condacolab.check()"
      ],
      "metadata": {
        "id": "P4oAvgySL9N8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing RAPIDS is now 'python rapidsai-csp-utils/colab/install_rapids.py <release> <packages>'\n",
        "# The <release> options are 'stable' and 'nightly'.  Leaving it blank or adding any other words will default to stable.\n",
        "!python rapidsai-csp-utils/colab/install_rapids.py stable\n",
        "import os\n",
        "os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n",
        "os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n",
        "os.environ['CONDA_PREFIX'] = '/usr/local'"
      ],
      "metadata": {
        "id": "W9QWZM0XMAqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CUML Random forest for 1 Million dataset with 43 columns**"
      ],
      "metadata": {
        "id": "7jybacR6Mqkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CUML Random forest for 1 Million dataset with 43 columns\n",
        "\n",
        "import timeit\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Change the classifier to test the speed of both sklearn and cuml\n",
        "from cuml.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import warnings\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "dataset=pd.read_csv('/content/drive/MyDrive/0_ColabDatasets/1M_dataset_43_columns.csv')\n",
        "\n",
        "print(list(dataset.columns))\n",
        "feature_cols= list(dataset.columns[:-1])\n",
        "\n",
        "target_cols=list(dataset.columns[-1:])\n",
        "#target_cols\n",
        "\n",
        "#split dataset in features and target variable\n",
        "X = dataset.drop('readmitted', axis=1) # Features\n",
        "y = dataset['readmitted'] # Target variable\n",
        "\n",
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "scaler = StandardScaler() \n",
        "X_train = scaler.fit_transform(X_train) \n",
        "X_test = scaler.transform(X_test) \n",
        "\n",
        "#Calculate start time\n",
        "start = timeit.default_timer()\n",
        "\n",
        "#Create a Gaussian Classifier\n",
        "clf=RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "clf.fit(X_train,y_train)\n",
        "\n",
        "#Calculate Stop time\n",
        "stop = timeit.default_timer()\n",
        "train_time= stop - start\n",
        "\n",
        "#Calculate start time\n",
        "start = timeit.default_timer()\n",
        "\n",
        "# Predict the model\n",
        "y_pred=clf.predict(X_test)\n",
        "\n",
        "#Calculate Stop time\n",
        "stop = timeit.default_timer()\n",
        "test_time= stop - start\n",
        "\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "\n",
        "cuml_1M_100_columns_train_time = train_time\n",
        "cuml_1M_100_columns_shape = dataset.shape"
      ],
      "metadata": {
        "id": "KdQp2OqMFw1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CUML Random forest for 3 Million dataset with 100 columns**"
      ],
      "metadata": {
        "id": "oIpT_nF5Mvbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CUML Random forest for 3 Million dataset with 100 columns\n",
        "\n",
        "import timeit\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Change the classifier to test the speed of both sklearn and cuml\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import warnings\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "dataset=pd.read_csv('/content/drive/MyDrive/0_ColabDatasets/3M_dataset_100_columns.csv')\n",
        "\n",
        "print(list(dataset.columns))\n",
        "feature_cols= list(dataset.columns[:-1])\n",
        "\n",
        "target_cols=list(dataset.columns[-1:])\n",
        "#target_cols\n",
        "\n",
        "#split dataset in features and target variable\n",
        "X = dataset.drop('readmitted', axis=1) # Features\n",
        "y = dataset['readmitted'] # Target variable\n",
        "\n",
        "# Split dataset into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "scaler = StandardScaler() \n",
        "X_train = scaler.fit_transform(X_train) \n",
        "X_test = scaler.transform(X_test) \n",
        "\n",
        "#Calculate start time\n",
        "start = timeit.default_timer()\n",
        "\n",
        "#Create a Gaussian Classifier\n",
        "clf=RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "clf.fit(X_train,y_train)\n",
        "\n",
        "#Calculate Stop time\n",
        "stop = timeit.default_timer()\n",
        "train_time= stop - start\n",
        "\n",
        "#Calculate start time\n",
        "start = timeit.default_timer()\n",
        "\n",
        "# Predict the model\n",
        "y_pred=clf.predict(X_test)\n",
        "\n",
        "#Calculate Stop time\n",
        "stop = timeit.default_timer()\n",
        "test_time= stop - start\n",
        "\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "\n",
        "cuml_3M_100_columns_train_time = train_time\n",
        "cuml_3M_100_columns_shape = dataset.shape"
      ],
      "metadata": {
        "id": "HhlHgYXXF8Zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Comparison between sklearn and cuml Random Forest**"
      ],
      "metadata": {
        "id": "kPF3wm96M0aX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparison between sklearn and cuml Random Forest\n",
        "print(\"System \" \" Package \" \" Dataset_shape \" \" Training_Time\")\n",
        "print(\"---------------------------------------------\")\n",
        "print(\"CPU    \", \"Sklearn\", sklearn_1M_43_columns_shape, sklearn_1M_43_columns_train_time)\n",
        "print(\"CPU    \", \"Sklearn\", sklearn_3M_100_columns_shape, sklearn_3M_100_columns_train_time)\n",
        "print(\"GPU    \", \"cuml\", cuml_1M_43_columns_shape, cuml_1M_43_columns_train_time)\n",
        "print(\"GPU    \", \"cuml\", cuml_3M_100_columns_shape, cuml_3M_100_columns_train_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uf_kXMSNEcDo",
        "outputId": "912c2ee9-d83c-470e-868b-38e55f309890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System  Package  Dataset_shape  Training_Time\n",
            "---------------------------------------------\n",
            "CPU     Sklearn (101766, 43) 16.24272100500002\n"
          ]
        }
      ]
    }
  ]
}